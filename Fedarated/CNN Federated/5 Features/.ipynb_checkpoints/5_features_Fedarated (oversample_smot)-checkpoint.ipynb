{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c72cc5-5e8c-4098-a296-7c9f8570ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, matthews_corrcoef\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f750175-efc8-4ffd-963a-bbe88992b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e506e51f-530f-4c24-80dc-729887c12ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"oversample_smote.csv\")\n",
    "\n",
    "selected_columns = ['hv104','hml18', 'shb70', 'ha53', 'shb13']\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[selected_columns]\n",
    "y = df['final_diabetes']\n",
    "\n",
    "# Convert X and y to numpy arrays if they are not already\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape data for the CNN model\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5607df5a-26fe-4bb5-889d-875c53078589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the robust CNN-LSTM hybrid model\n",
    "def cnn_lstm(learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, padding=\"same\", input_shape=(5, 1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ff2954-73e7-4c89-8014-d2619bcb8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred_binary)\n",
    "    precision = precision_score(y_val, y_val_pred_binary)\n",
    "    recall = recall_score(y_val, y_val_pred_binary)\n",
    "    f1 = f1_score(y_val, y_val_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    cm = confusion_matrix(y_val, y_val_pred_binary)\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    kappa = cohen_kappa_score(y_val, y_val_pred_binary)\n",
    "    mcc = matthews_corrcoef(y_val, y_val_pred_binary)\n",
    "\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc, specificity, kappa, mcc, cm]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6b1b7f-f40e-46a5-8492-606d94dd5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (949976, 5, 1), Test shape: (474988, 5, 1)\n",
      "Train shape: (949976, 5, 1), Test shape: (474988, 5, 1)\n",
      "Train shape: (949976, 5, 1), Test shape: (474988, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class FederatedData:\n",
    "    def __init__(self, X, y, num_clients, test_size=0.2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_clients = num_clients\n",
    "        self.test_size = test_size\n",
    "        self.partitions = []\n",
    "        self.create_partitions()\n",
    "\n",
    "    def create_partitions(self):\n",
    "        skf = StratifiedKFold(n_splits=self.num_clients, shuffle=True, random_state=42)\n",
    "        for train_index, test_index in skf.split(self.X, self.y):\n",
    "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "            y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "            self.partitions.append((X_train, y_train, X_test, y_test))\n",
    "            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "    def get_training_and_validation_data(self, client_idx):\n",
    "        if client_idx < 0 or client_idx >= len(self.partitions):\n",
    "            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n",
    "        partition_X_train, partition_y_train, partition_X_test, partition_y_test = self.partitions[client_idx]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(partition_X_train, partition_y_train, test_size=0.2, stratify=partition_y_train, random_state=42)\n",
    "        return X_train, X_val, y_train, y_val, partition_X_test, partition_y_test\n",
    "\n",
    "# Initialize federated data\n",
    "federated_data = FederatedData(X, y, num_clients=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082761c-85ed-48fc-9204-d2b4026c567b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Epoch 1:\n",
      "Epoch 1/50\n",
      "2969/2969 [==============================] - 268s 86ms/step - loss: 0.0616 - accuracy: 0.9742 - val_loss: 0.0671 - val_accuracy: 0.9713\n",
      "Epoch 2/50\n",
      "2969/2969 [==============================] - 257s 87ms/step - loss: 0.0556 - accuracy: 0.9758 - val_loss: 0.0516 - val_accuracy: 0.9767\n",
      "Epoch 3/50\n",
      "2969/2969 [==============================] - 258s 87ms/step - loss: 0.0542 - accuracy: 0.9761 - val_loss: 0.0499 - val_accuracy: 0.9772\n",
      "Epoch 4/50\n",
      "2969/2969 [==============================] - 259s 87ms/step - loss: 0.0535 - accuracy: 0.9764 - val_loss: 0.0498 - val_accuracy: 0.9775\n",
      "Epoch 5/50\n",
      "2969/2969 [==============================] - 258s 87ms/step - loss: 0.0530 - accuracy: 0.9765 - val_loss: 0.0498 - val_accuracy: 0.9769\n",
      "Epoch 6/50\n",
      "2969/2969 [==============================] - 251s 84ms/step - loss: 0.0526 - accuracy: 0.9765 - val_loss: 0.0503 - val_accuracy: 0.9771\n",
      "Epoch 7/50\n",
      "2969/2969 [==============================] - 117s 39ms/step - loss: 0.0523 - accuracy: 0.9767 - val_loss: 0.0490 - val_accuracy: 0.9777\n",
      "Epoch 8/50\n",
      "2969/2969 [==============================] - 257s 87ms/step - loss: 0.0521 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9776\n",
      "Epoch 9/50\n",
      "2969/2969 [==============================] - 258s 87ms/step - loss: 0.0519 - accuracy: 0.9767 - val_loss: 0.0495 - val_accuracy: 0.9773\n",
      "Epoch 10/50\n",
      "2969/2969 [==============================] - 259s 87ms/step - loss: 0.0517 - accuracy: 0.9769 - val_loss: 0.0489 - val_accuracy: 0.9778\n",
      "Epoch 11/50\n",
      "2969/2969 [==============================] - 258s 87ms/step - loss: 0.0516 - accuracy: 0.9770 - val_loss: 0.0506 - val_accuracy: 0.9772\n",
      "Epoch 12/50\n",
      "2969/2969 [==============================] - 258s 87ms/step - loss: 0.0513 - accuracy: 0.9770 - val_loss: 0.0495 - val_accuracy: 0.9773\n",
      "Epoch 13/50\n",
      "2969/2969 [==============================] - 256s 86ms/step - loss: 0.0515 - accuracy: 0.9769 - val_loss: 0.0497 - val_accuracy: 0.9775\n",
      "Epoch 14/50\n",
      "2969/2969 [==============================] - 248s 84ms/step - loss: 0.0513 - accuracy: 0.9769 - val_loss: 0.0496 - val_accuracy: 0.9774\n",
      "Epoch 15/50\n",
      "2969/2969 [==============================] - 114s 38ms/step - loss: 0.0513 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9777\n",
      "Epoch 16/50\n",
      "2969/2969 [==============================] - 254s 86ms/step - loss: 0.0511 - accuracy: 0.9770 - val_loss: 0.0491 - val_accuracy: 0.9775\n",
      "Epoch 17/50\n",
      "2969/2969 [==============================] - 254s 86ms/step - loss: 0.0510 - accuracy: 0.9771 - val_loss: 0.0489 - val_accuracy: 0.9776\n",
      "Epoch 18/50\n",
      "2969/2969 [==============================] - 252s 85ms/step - loss: 0.0510 - accuracy: 0.9771 - val_loss: 0.0492 - val_accuracy: 0.9774\n",
      "Epoch 19/50\n",
      "2969/2969 [==============================] - 255s 86ms/step - loss: 0.0511 - accuracy: 0.9771 - val_loss: 0.0492 - val_accuracy: 0.9774\n",
      "Epoch 20/50\n",
      "2969/2969 [==============================] - 176s 59ms/step - loss: 0.0510 - accuracy: 0.9771 - val_loss: 0.0496 - val_accuracy: 0.9774\n",
      "Epoch 21/50\n",
      "2969/2969 [==============================] - 102s 34ms/step - loss: 0.0508 - accuracy: 0.9771 - val_loss: 0.0488 - val_accuracy: 0.9777\n",
      "Epoch 22/50\n",
      "2969/2969 [==============================] - 109s 37ms/step - loss: 0.0507 - accuracy: 0.9771 - val_loss: 0.0491 - val_accuracy: 0.9777\n",
      "Epoch 23/50\n",
      "2969/2969 [==============================] - 110s 37ms/step - loss: 0.0507 - accuracy: 0.9771 - val_loss: 0.0493 - val_accuracy: 0.9776\n",
      "Epoch 24/50\n",
      "2969/2969 [==============================] - 263s 89ms/step - loss: 0.0507 - accuracy: 0.9772 - val_loss: 0.0492 - val_accuracy: 0.9776\n",
      "Epoch 25/50\n",
      "2969/2969 [==============================] - 259s 87ms/step - loss: 0.0506 - accuracy: 0.9771 - val_loss: 0.0497 - val_accuracy: 0.9773\n",
      "Epoch 26/50\n",
      "2969/2969 [==============================] - 255s 86ms/step - loss: 0.0507 - accuracy: 0.9771 - val_loss: 0.0489 - val_accuracy: 0.9774\n",
      "Epoch 27/50\n",
      "2969/2969 [==============================] - 263s 89ms/step - loss: 0.0505 - accuracy: 0.9773 - val_loss: 0.0487 - val_accuracy: 0.9776\n",
      "Epoch 28/50\n",
      "2969/2969 [==============================] - 266s 89ms/step - loss: 0.0505 - accuracy: 0.9771 - val_loss: 0.0488 - val_accuracy: 0.9776\n",
      "Epoch 29/50\n",
      "2969/2969 [==============================] - 133s 45ms/step - loss: 0.0504 - accuracy: 0.9772 - val_loss: 0.0485 - val_accuracy: 0.9776\n",
      "Epoch 30/50\n",
      "2969/2969 [==============================] - 110s 37ms/step - loss: 0.0505 - accuracy: 0.9773 - val_loss: 0.0491 - val_accuracy: 0.9774\n",
      "Epoch 31/50\n",
      "2969/2969 [==============================] - 134s 45ms/step - loss: 0.0504 - accuracy: 0.9772 - val_loss: 0.0486 - val_accuracy: 0.9777\n",
      "Epoch 32/50\n",
      "1351/2969 [============>.................] - ETA: 2:19 - loss: 0.0506 - accuracy: 0.9772"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "# Custom CSV Logger\n",
    "class CustomCSVLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filename, separator=',', append=False):\n",
    "        self.sep = separator\n",
    "        self.filename = filename\n",
    "        self.append = append\n",
    "        self.file = None\n",
    "        self.writer = None\n",
    "        self.keys = None\n",
    "        super(CustomCSVLogger, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not self.append and os.path.isfile(self.filename):\n",
    "            os.remove(self.filename)\n",
    "        mode = 'a' if self.append else 'w'\n",
    "        self.file = open(self.filename, mode, newline='')\n",
    "        self.writer = csv.writer(self.file, delimiter=self.sep)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.keys is None:\n",
    "            self.keys = sorted(logs.keys())\n",
    "            self.writer.writerow(self.keys)\n",
    "        row = [logs.get(key) for key in self.keys]\n",
    "        self.writer.writerow(row)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "# Initialize global model\n",
    "input_shape = X.shape[1:]\n",
    "global_model = cnn_lstm()\n",
    "num_clients = 3\n",
    "local_epochs = 1\n",
    "\n",
    "# CSV file to store performance metrics\n",
    "csv_filename = \"5_features_federated_learning_metrics.csv\"\n",
    "header = ['Local Epoch', 'Client', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'Specificity', 'Kappa', 'MCC', 'Confusion Matrix']\n",
    "with open(csv_filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Federated training process\n",
    "for local_epoch in range(1, local_epochs + 1):\n",
    "    print(f\"Local Epoch {local_epoch}:\")\n",
    "    client_models = []\n",
    "\n",
    "    # Create a directory for the current local epoch\n",
    "    epoch_dir = f\"epoch_{local_epoch}\"\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    for client in range(num_clients):\n",
    "        X_train, X_val, y_train, y_val, X_test, y_test = federated_data.get_training_and_validation_data(client)\n",
    "        client_model = cnn_lstm()\n",
    "        client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n",
    "\n",
    "        with tf.device('/GPU:0'):\n",
    "            log_dir = os.path.join(epoch_dir, \"logs/profile/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "            csv_logger = CustomCSVLogger(os.path.join(epoch_dir, f\"training_client_{client+1}_epoch_{local_epoch}.csv\"))\n",
    "\n",
    "            history = client_model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=50,  # Train for 10 epochs (as a placeholder, adjust as needed)\n",
    "                batch_size=256,\n",
    "                validation_data=(X_val, y_val),\n",
    "                verbose=1,\n",
    "                callbacks=[tensorboard_callback, csv_logger]\n",
    "            )\n",
    "\n",
    "        # Evaluate the client model\n",
    "        client_metrics = evaluate_model(client_model, X_val, y_val)\n",
    "        print(f\"Client {client + 1} - Metrics after Local Epoch {local_epoch}: {client_metrics}\")\n",
    "\n",
    "        # Save client model performance to CSV\n",
    "        with open(csv_filename, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([local_epoch, client + 1] + client_metrics[:-1] + [np.array2string(client_metrics[-1], separator=',')])\n",
    "\n",
    "        client_models.append(client_model)\n",
    "\n",
    "    # Aggregating weights from client models to update global model\n",
    "    global_weights = global_model.get_weights()\n",
    "    new_weights = [client_model.get_weights() for client_model in client_models]\n",
    "\n",
    "    averaged_weights = [np.mean(np.array([client_weight[layer] for client_weight in new_weights]), axis=0) for layer in range(len(global_weights))]\n",
    "    global_model.set_weights(averaged_weights)\n",
    "\n",
    "# Save the global model\n",
    "global_model.save(\"5_features_global_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dd35a-77a8-414f-a915-6205460a535a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e441ece-44a2-44e8-ac75-c4c4c072f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
