{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c72cc5-5e8c-4098-a296-7c9f8570ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, matthews_corrcoef\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f750175-efc8-4ffd-963a-bbe88992b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e506e51f-530f-4c24-80dc-729887c12ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"oversample_smote.csv\")\n",
    "\n",
    "selected_columns = ['hv104','hml18', 'shb70', 'ha53', 'shb13']\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[selected_columns]\n",
    "y = df['final_diabetes']\n",
    "\n",
    "# Convert X and y to numpy arrays if they are not already\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape data for the CNN model\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5607df5a-26fe-4bb5-889d-875c53078589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model architecture using a function\n",
    "def cnnmodel():\n",
    "    clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', input_shape=(5, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=3, strides=1, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid', dtype='float32'))  # Ensure output layer is float32\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)  # Static learning rate of 0.001\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ff2954-73e7-4c89-8014-d2619bcb8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred_binary)\n",
    "    precision = precision_score(y_val, y_val_pred_binary)\n",
    "    recall = recall_score(y_val, y_val_pred_binary)\n",
    "    f1 = f1_score(y_val, y_val_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    cm = confusion_matrix(y_val, y_val_pred_binary)\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    kappa = cohen_kappa_score(y_val, y_val_pred_binary)\n",
    "    mcc = matthews_corrcoef(y_val, y_val_pred_binary)\n",
    "\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc, specificity, kappa, mcc, cm]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6b1b7f-f40e-46a5-8492-606d94dd5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (949976, 5, 1), Test shape: (474988, 5, 1)\n",
      "Train shape: (949976, 5, 1), Test shape: (474988, 5, 1)\n",
      "Train shape: (949976, 5, 1), Test shape: (474988, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class FederatedData:\n",
    "    def __init__(self, X, y, num_clients, test_size=0.2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_clients = num_clients\n",
    "        self.test_size = test_size\n",
    "        self.partitions = []\n",
    "        self.create_partitions()\n",
    "\n",
    "    def create_partitions(self):\n",
    "        skf = StratifiedKFold(n_splits=self.num_clients, shuffle=True, random_state=42)\n",
    "        for train_index, test_index in skf.split(self.X, self.y):\n",
    "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "            y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "            self.partitions.append((X_train, y_train, X_test, y_test))\n",
    "            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "    def get_training_and_validation_data(self, client_idx):\n",
    "        if client_idx < 0 or client_idx >= len(self.partitions):\n",
    "            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n",
    "        partition_X_train, partition_y_train, partition_X_test, partition_y_test = self.partitions[client_idx]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(partition_X_train, partition_y_train, test_size=0.2, stratify=partition_y_train, random_state=42)\n",
    "        return X_train, X_val, y_train, y_val, partition_X_test, partition_y_test\n",
    "\n",
    "# Initialize federated data\n",
    "federated_data = FederatedData(X, y, num_clients=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082761c-85ed-48fc-9204-d2b4026c567b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Epoch 1:\n",
      "Epoch 1/50\n",
      " 292/2969 [=>............................] - ETA: 3:33 - loss: 0.0896 - accuracy: 0.9706"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "# Custom CSV Logger\n",
    "class CustomCSVLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filename, separator=',', append=False):\n",
    "        self.sep = separator\n",
    "        self.filename = filename\n",
    "        self.append = append\n",
    "        self.file = None\n",
    "        self.writer = None\n",
    "        self.keys = None\n",
    "        super(CustomCSVLogger, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not self.append and os.path.isfile(self.filename):\n",
    "            os.remove(self.filename)\n",
    "        mode = 'a' if self.append else 'w'\n",
    "        self.file = open(self.filename, mode, newline='')\n",
    "        self.writer = csv.writer(self.file, delimiter=self.sep)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.keys is None:\n",
    "            self.keys = sorted(logs.keys())\n",
    "            self.writer.writerow(self.keys)\n",
    "        row = [logs.get(key) for key in self.keys]\n",
    "        self.writer.writerow(row)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "# Initialize global model\n",
    "input_shape = X.shape[1:]\n",
    "global_model = cnnmodel()\n",
    "num_clients = 3\n",
    "local_epochs = 1\n",
    "\n",
    "# CSV file to store performance metrics\n",
    "csv_filename = \"CNN_5_features_federated_learning_metrics.csv\"\n",
    "header = ['Local Epoch', 'Client', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'Specificity', 'Kappa', 'MCC', 'Confusion Matrix']\n",
    "with open(csv_filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Federated training process\n",
    "for local_epoch in range(1, local_epochs + 1):\n",
    "    print(f\"Local Epoch {local_epoch}:\")\n",
    "    client_models = []\n",
    "\n",
    "    # Create a directory for the current local epoch\n",
    "    epoch_dir = f\"epoch_{local_epoch}\"\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    for client in range(num_clients):\n",
    "        X_train, X_val, y_train, y_val, X_test, y_test = federated_data.get_training_and_validation_data(client)\n",
    "        client_model = cnnmodel()\n",
    "        client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n",
    "\n",
    "        with tf.device('/GPU:0'):\n",
    "            log_dir = os.path.join(epoch_dir, \"logs/profile/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "            csv_logger = CustomCSVLogger(os.path.join(epoch_dir, f\"training_client_{client+1}_epoch_{local_epoch}.csv\"))\n",
    "\n",
    "            history = client_model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=50,  # Train for 10 epochs (as a placeholder, adjust as needed)\n",
    "                batch_size=256,\n",
    "                validation_data=(X_val, y_val),\n",
    "                verbose=1,\n",
    "                callbacks=[tensorboard_callback, csv_logger]\n",
    "            )\n",
    "\n",
    "        # Evaluate the client model\n",
    "        client_metrics = evaluate_model(client_model, X_val, y_val)\n",
    "        print(f\"Client {client + 1} - Metrics after Local Epoch {local_epoch}: {client_metrics}\")\n",
    "\n",
    "        # Save client model performance to CSV\n",
    "        with open(csv_filename, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([local_epoch, client + 1] + client_metrics[:-1] + [np.array2string(client_metrics[-1], separator=',')])\n",
    "\n",
    "        client_models.append(client_model)\n",
    "\n",
    "    # Aggregating weights from client models to update global model\n",
    "    global_weights = global_model.get_weights()\n",
    "    new_weights = [client_model.get_weights() for client_model in client_models]\n",
    "\n",
    "    averaged_weights = [np.mean(np.array([client_weight[layer] for client_weight in new_weights]), axis=0) for layer in range(len(global_weights))]\n",
    "    global_model.set_weights(averaged_weights)\n",
    "\n",
    "# Save the global model\n",
    "global_model.save(\"CNN_5_features_global_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dd35a-77a8-414f-a915-6205460a535a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e441ece-44a2-44e8-ac75-c4c4c072f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
